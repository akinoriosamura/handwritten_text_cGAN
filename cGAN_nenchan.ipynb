{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pULFh4W419LH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pULFh4W419LH",
    "outputId": "65d8c1d7-f21c-40ec-bb95-e1381520e491"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe34f25",
   "metadata": {
    "id": "efe34f25"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8YP2rCYhwAG3",
   "metadata": {
    "id": "8YP2rCYhwAG3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6iEb-levoi-N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6iEb-levoi-N",
    "outputId": "a3ff2000-0d89-4621-e6ff-3899213ed62e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k49-test-imgs.npz    k49-train-imgs.npz\r\n",
      "k49-test-labels.npz  k49-train-labels.npz\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "zM_5LAWZwBye",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "zM_5LAWZwBye",
    "outputId": "c9ba14f2-775f-4270-d3b6-658720ee30a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport zipfile\\n\\nhiragana73_path = '../dataset/hira_gen/hiragana73.zip'\\nwith zipfile.ZipFile(hiragana73_path) as existing_zip:\\n    existing_zip.extractall(hiragana73_path[:-4])\\nhiragana73s = os.listdir(hiragana73_path[:-4])\\nhiragana73_dirs = [f for f in hiragana73s if os.path.isdir(os.path.join(hiragana73_path[:-4], f))]\\nprint(hiragana73_dirs) \\n\\nhiragana_imgs_path = '/content/drive/MyDrive/hira_gen/hiragana_imgs.zip'\\nwith zipfile.ZipFile(hiragana_imgs_path) as existing_zip:\\n    existing_zip.extractall(hiragana_imgs_path[:-4])\\nhiragana_imgs = os.listdir(hiragana_imgs_path[:-4])\\nhiragana_img_dirs = [f for f in hiragana_imgs if os.path.isdir(os.path.join(hiragana_imgs_path[:-4], f))]\\nprint(hiragana_img_dirs) \\n\\n# get label\\n# あーん\\nall_label_classes = set([os.path.basename(d) for d in hiragana73_dirs] + [os.path.basename(d) for d in hiragana_img_dirs])\\n# 0-\\nall_labels = [i for i in range(len(all_label_classes))]\\nprint(all_label_classes)\\nprint(len(all_label_classes))\\nprint(all_labels)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import zipfile\n",
    "\n",
    "hiragana73_path = '../dataset/hira_gen/hiragana73.zip'\n",
    "with zipfile.ZipFile(hiragana73_path) as existing_zip:\n",
    "    existing_zip.extractall(hiragana73_path[:-4])\n",
    "hiragana73s = os.listdir(hiragana73_path[:-4])\n",
    "hiragana73_dirs = [f for f in hiragana73s if os.path.isdir(os.path.join(hiragana73_path[:-4], f))]\n",
    "print(hiragana73_dirs) \n",
    "\n",
    "hiragana_imgs_path = '/content/drive/MyDrive/hira_gen/hiragana_imgs.zip'\n",
    "with zipfile.ZipFile(hiragana_imgs_path) as existing_zip:\n",
    "    existing_zip.extractall(hiragana_imgs_path[:-4])\n",
    "hiragana_imgs = os.listdir(hiragana_imgs_path[:-4])\n",
    "hiragana_img_dirs = [f for f in hiragana_imgs if os.path.isdir(os.path.join(hiragana_imgs_path[:-4], f))]\n",
    "print(hiragana_img_dirs) \n",
    "\n",
    "# get label\n",
    "# あーん\n",
    "all_label_classes = set([os.path.basename(d) for d in hiragana73_dirs] + [os.path.basename(d) for d in hiragana_img_dirs])\n",
    "# 0-\n",
    "all_labels = [i for i in range(len(all_label_classes))]\n",
    "print(all_label_classes)\n",
    "print(len(all_label_classes))\n",
    "print(all_labels)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbeeada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'U3042': 'あ', 'U3044': 'い', 'U3046': 'う', 'U3048': 'え', 'U304A': 'お', 'U304B': 'か', 'U304C': 'が', 'U304D': 'き', 'U304E': 'ぎ', 'U304F': 'く', 'U3050': 'ぐ', 'U3051': 'け', 'U3052': 'げ', 'U3053': 'こ', 'U3054': 'ご', 'U3055': 'さ', 'U3056': 'ざ', 'U3057': 'し', 'U3058': 'じ', 'U3059': 'す', 'U305A': 'ず', 'U305B': 'せ', 'U305C': 'ぜ', 'U305D': 'そ', 'U305E': 'ぞ', 'U305F': 'た', 'U3060': 'だ', 'U3061': 'ち', 'U3062': 'ぢ', 'U3064': 'つ', 'U3065': 'づ', 'U3066': 'て', 'U3067': 'で', 'U3068': 'と', 'U3069': 'ど', 'U306A': 'な', 'U306B': 'に', 'U306C': 'ぬ', 'U306D': 'ね', 'U306E': 'の', 'U306F': 'は', 'U3070': 'ば', 'U3071': 'ぱ', 'U3072': 'ひ', 'U3073': 'び', 'U3074': 'ぴ', 'U3075': 'ふ', 'U3076': 'ぶ', 'U3077': 'ぷ', 'U3078': 'へ', 'U3079': 'べ', 'U307A': 'ぺ', 'U307B': 'ほ', 'U307C': 'ぼ', 'U307D': 'ぽ', 'U307E': 'ま', 'U307F': 'み', 'U3080': 'む', 'U3081': 'め', 'U3082': 'も', 'U3084': 'や', 'U3086': 'ゆ', 'U3088': 'よ', 'U3089': 'ら', 'U308A': 'り', 'U308B': 'る', 'U308C': 'れ', 'U308D': 'ろ', 'U308F': 'わ', 'U3090': 'ゐ', 'U3091': 'ゑ', 'U3092': 'を', 'U3093': 'ん'}\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "# create hiragana73 map dict\n",
    "hiragana73_map = {}\n",
    "with open('./hiragana_mojigazo_map.txt') as hm:\n",
    "    lines = hm.readlines()\n",
    "    for l in lines:\n",
    "        map_line = l.strip().split(',')\n",
    "        hiragana73_map[map_line[1]] = map_line[0]\n",
    "print(hiragana73_map)\n",
    "print(len(hiragana73_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d7c0c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'げ'=='げ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e008e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '../dataset/hiragana73'\n",
    "for fn in os.listdir(basedir):\n",
    "    # print(fn)\n",
    "    if not os.path.isdir(os.path.join(basedir, fn)):\n",
    "        continue # Not a directory\n",
    "    if fn in hiragana73_map.keys():\n",
    "        newfn  = hiragana73_map[fn]\n",
    "        # print(newfn)\n",
    "        os.rename(os.path.join(basedir, fn), os.path.join(basedir, newfn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce780b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ぱ', 'え', 'た', 'つ', 'ぷ', 'げ', 'そ', 'ろ', 'だ', 'ぺ', 'り', 'ご', 'ぎ', 'ず', 'よ', 'ね', 'ば', 'ひ', 'ゑ', 'ま', 'や', 'め', 'ん', 'を', 'は', 'む', 'ほ', 'ぶ', 'ふ', 'び', 'と', 'の', 'う', 'て', 'ざ', 'が', 'へ', 'く', 'ぼ', 'ゐ', 'も', 'に', 'わ', 'べ', 'ぬ', 'じ', 'れ', 'ら', 'ぢ', 'す', 'づ', 'ぞ', 'ち', 'け', 'お', 'な', 'い', 'で', 'あ', 'ど', 'き', 'ぜ', 'ぐ', 'ぽ', 'さ', 'こ', 'か', 'せ', 'み', 'る', 'し', 'ゆ', 'ぴ']\n",
      "73\n",
      "['ゅ', 'ぱ', 'え', 'た', 'つ', 'ぷ', 'げ', 'そ', 'ろ', 'ゃ', 'だ', 'ぺ', 'り', 'ご', 'ぎ', 'ず', 'よ', 'ね', 'ば', 'ひ', 'ぃ', 'ゑ', 'ま', 'ぁ', 'や', 'め', 'ん', 'を', 'は', 'む', 'ほ', 'ぶ', 'ふ', 'ぉ', 'び', 'と', 'の', 'っ', 'う', 'て', 'ざ', 'が', 'へ', 'ぅ', 'く', 'ぼ', 'ゐ', 'も', 'に', 'わ', 'ゝ', 'べ', 'ぬ', 'ぇ', 'じ', 'れ', 'ら', 'ぢ', 'ょ', 'す', 'づ', 'ぞ', 'ち', 'け', 'お', 'な', 'い', 'で', 'あ', 'ど', 'き', 'ぜ', 'ゎ', 'ぐ', 'ぽ', 'さ', 'こ', 'か', 'せ', 'み', 'ゞ', 'る', 'し', 'ゆ', 'ぴ']\n",
      "85\n",
      "['ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'ぇ', 'え', 'ぉ', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず', 'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'づ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ', 'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'ぺ', 'ほ', 'ぼ', 'ぽ', 'ま', 'み', 'む', 'め', 'も', 'ゃ', 'や', 'ゅ', 'ゆ', 'ょ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'ゎ', 'わ', 'ゐ', 'ゑ', 'を', 'ん', 'ゝ', 'ゞ']\n",
      "85\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]\n"
     ]
    }
   ],
   "source": [
    "hiragana73_path = '../dataset/hiragana73'\n",
    "\n",
    "hiragana73s = os.listdir(hiragana73_path)\n",
    "hiragana73_dirs = []\n",
    "for f in hiragana73s:\n",
    "    # print(f)\n",
    "    if os.path.isdir(os.path.join(hiragana73_path, f)):\n",
    "        hiragana73_dirs.append(f)\n",
    "print(hiragana73_dirs)\n",
    "print(len(hiragana73_dirs))\n",
    "\n",
    "hiragana_imgs_path = '../dataset/hiragana_imgs'\n",
    "\n",
    "hiragana_imgs = os.listdir(hiragana_imgs_path)\n",
    "hiragana_img_dirs = [f for f in hiragana_imgs if os.path.isdir(os.path.join(hiragana_imgs_path, f))]\n",
    "print(hiragana_img_dirs) \n",
    "print(len(hiragana_img_dirs))\n",
    "\n",
    "# get label\n",
    "# あーん\n",
    "all_label_classes = list(set([os.path.basename(d) for d in hiragana73_dirs] + [os.path.basename(d) for d in hiragana_img_dirs]))\n",
    "all_label_classes.sort()\n",
    "# 0-\n",
    "all_labels = [i for i in range(len(all_label_classes))]\n",
    "num_label = len(all_label_classes)\n",
    "print(all_label_classes)\n",
    "print(num_label)\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "xPICTLr-w_yC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "xPICTLr-w_yC",
    "outputId": "e79130cc-5333-4754-c949-8658d67c3f06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "380\n",
      "380\n",
      "ぁ\n",
      "380\n",
      "1208\n",
      "386\n",
      "1594\n",
      "あ\n",
      "1594\n",
      "0\n",
      "380\n",
      "380\n",
      "ぃ\n",
      "380\n",
      "1122\n",
      "386\n",
      "1508\n",
      "い\n",
      "1508\n",
      "0\n",
      "380\n",
      "380\n",
      "ぅ\n",
      "380\n",
      "1148\n",
      "386\n",
      "1534\n",
      "う\n",
      "1534\n",
      "0\n",
      "380\n",
      "380\n",
      "ぇ\n",
      "380\n",
      "1077\n",
      "386\n",
      "1463\n",
      "え\n",
      "1463\n",
      "0\n",
      "380\n",
      "380\n",
      "ぉ\n",
      "380\n",
      "1283\n",
      "386\n",
      "1669\n",
      "お\n",
      "1669\n",
      "1259\n",
      "386\n",
      "1645\n",
      "か\n",
      "1645\n",
      "1200\n",
      "384\n",
      "1584\n",
      "が\n",
      "1584\n",
      "1121\n",
      "386\n",
      "1507\n",
      "き\n",
      "1507\n",
      "1091\n",
      "384\n",
      "1475\n",
      "ぎ\n",
      "1475\n",
      "1266\n",
      "386\n",
      "1652\n",
      "く\n",
      "1652\n",
      "1043\n",
      "384\n",
      "1427\n",
      "ぐ\n",
      "1427\n",
      "1155\n",
      "386\n",
      "1541\n",
      "け\n",
      "1541\n",
      "1058\n",
      "384\n",
      "1442\n",
      "げ\n",
      "1442\n",
      "1115\n",
      "391\n",
      "1506\n",
      "こ\n",
      "1506\n",
      "1078\n",
      "384\n",
      "1462\n",
      "ご\n",
      "1462\n",
      "1261\n",
      "386\n",
      "1647\n",
      "さ\n",
      "1647\n",
      "1070\n",
      "379\n",
      "1449\n",
      "ざ\n",
      "1449\n",
      "1285\n",
      "386\n",
      "1671\n",
      "し\n",
      "1671\n",
      "1099\n",
      "379\n",
      "1478\n",
      "じ\n",
      "1478\n",
      "1282\n",
      "386\n",
      "1668\n",
      "す\n",
      "1668\n",
      "1046\n",
      "379\n",
      "1425\n",
      "ず\n",
      "1425\n",
      "1165\n",
      "386\n",
      "1551\n",
      "せ\n",
      "1551\n",
      "1115\n",
      "379\n",
      "1494\n",
      "ぜ\n",
      "1494\n",
      "1285\n",
      "386\n",
      "1671\n",
      "そ\n",
      "1671\n",
      "1066\n",
      "379\n",
      "1445\n",
      "ぞ\n",
      "1445\n",
      "1285\n",
      "386\n",
      "1671\n",
      "た\n",
      "1671\n",
      "1116\n",
      "379\n",
      "1495\n",
      "だ\n",
      "1495\n",
      "1052\n",
      "386\n",
      "1438\n",
      "ち\n",
      "1438\n",
      "1132\n",
      "379\n",
      "1511\n",
      "ぢ\n",
      "1511\n",
      "0\n",
      "381\n",
      "381\n",
      "っ\n",
      "381\n",
      "1142\n",
      "386\n",
      "1528\n",
      "つ\n",
      "1528\n",
      "1080\n",
      "379\n",
      "1459\n",
      "づ\n",
      "1459\n",
      "1213\n",
      "386\n",
      "1599\n",
      "て\n",
      "1599\n",
      "1178\n",
      "379\n",
      "1557\n",
      "で\n",
      "1557\n",
      "1184\n",
      "386\n",
      "1570\n",
      "と\n",
      "1570\n",
      "1134\n",
      "379\n",
      "1513\n",
      "ど\n",
      "1513\n",
      "1233\n",
      "386\n",
      "1619\n",
      "な\n",
      "1619\n",
      "1260\n",
      "386\n",
      "1646\n",
      "に\n",
      "1646\n",
      "1093\n",
      "386\n",
      "1479\n",
      "ぬ\n",
      "1479\n",
      "1126\n",
      "386\n",
      "1512\n",
      "ね\n",
      "1512\n",
      "1160\n",
      "386\n",
      "1546\n",
      "の\n",
      "1546\n",
      "1247\n",
      "386\n",
      "1633\n",
      "は\n",
      "1633\n",
      "1105\n",
      "379\n",
      "1484\n",
      "ば\n",
      "1484\n",
      "262\n",
      "379\n",
      "641\n",
      "ぱ\n",
      "641\n",
      "1074\n",
      "386\n",
      "1460\n",
      "ひ\n",
      "1460\n",
      "1045\n",
      "379\n",
      "1424\n",
      "び\n",
      "1424\n",
      "126\n",
      "379\n",
      "505\n",
      "ぴ\n",
      "505\n",
      "1285\n",
      "386\n",
      "1671\n",
      "ふ\n",
      "1671\n",
      "1149\n",
      "379\n",
      "1528\n",
      "ぶ\n",
      "1528\n",
      "112\n",
      "379\n",
      "491\n",
      "ぷ\n",
      "491\n",
      "1114\n",
      "386\n",
      "1500\n",
      "へ\n",
      "1500\n",
      "1109\n",
      "379\n",
      "1488\n",
      "べ\n",
      "1488\n",
      "268\n",
      "379\n",
      "647\n",
      "ぺ\n",
      "647\n",
      "1115\n",
      "386\n",
      "1501\n",
      "ほ\n",
      "1501\n",
      "1044\n",
      "379\n",
      "1423\n",
      "ぼ\n",
      "1423\n",
      "261\n",
      "379\n",
      "640\n",
      "ぽ\n",
      "640\n",
      "1285\n",
      "391\n",
      "1676\n",
      "ま\n",
      "1676\n",
      "1142\n",
      "386\n",
      "1528\n",
      "み\n",
      "1528\n",
      "1058\n",
      "386\n",
      "1444\n",
      "む\n",
      "1444\n",
      "1233\n",
      "386\n",
      "1619\n",
      "め\n",
      "1619\n",
      "1187\n",
      "386\n",
      "1573\n",
      "も\n",
      "1573\n",
      "0\n",
      "381\n",
      "381\n",
      "ゃ\n",
      "381\n",
      "1285\n",
      "386\n",
      "1671\n",
      "や\n",
      "1671\n",
      "0\n",
      "381\n",
      "381\n",
      "ゅ\n",
      "381\n",
      "1282\n",
      "391\n",
      "1673\n",
      "ゆ\n",
      "1673\n",
      "0\n",
      "381\n",
      "381\n",
      "ょ\n",
      "381\n",
      "1166\n",
      "391\n",
      "1557\n",
      "よ\n",
      "1557\n",
      "1114\n",
      "386\n",
      "1500\n",
      "ら\n",
      "1500\n",
      "1244\n",
      "386\n",
      "1630\n",
      "り\n",
      "1630\n",
      "1190\n",
      "386\n",
      "1576\n",
      "る\n",
      "1576\n",
      "1238\n",
      "386\n",
      "1624\n",
      "れ\n",
      "1624\n",
      "1069\n",
      "386\n",
      "1455\n",
      "ろ\n",
      "1455\n",
      "0\n",
      "378\n",
      "378\n",
      "ゎ\n",
      "378\n",
      "1283\n",
      "386\n",
      "1669\n",
      "わ\n",
      "1669\n",
      "1053\n",
      "380\n",
      "1433\n",
      "ゐ\n",
      "1433\n",
      "1030\n",
      "380\n",
      "1410\n",
      "ゑ\n",
      "1410\n",
      "1254\n",
      "386\n",
      "1640\n",
      "を\n",
      "1640\n",
      "1285\n",
      "386\n",
      "1671\n",
      "ん\n",
      "1671\n",
      "0\n",
      "368\n",
      "368\n",
      "ゝ\n",
      "368\n",
      "0\n",
      "364\n",
      "364\n",
      "ゞ\n",
      "364\n",
      "{'ぁ': '0', 'あ': '1', 'ぃ': '2', 'い': '3', 'ぅ': '4', 'う': '5', 'ぇ': '6', 'え': '7', 'ぉ': '8', 'お': '9', 'か': '10', 'が': '11', 'き': '12', 'ぎ': '13', 'く': '14', 'ぐ': '15', 'け': '16', 'げ': '17', 'こ': '18', 'ご': '19', 'さ': '20', 'ざ': '21', 'し': '22', 'じ': '23', 'す': '24', 'ず': '25', 'せ': '26', 'ぜ': '27', 'そ': '28', 'ぞ': '29', 'た': '30', 'だ': '31', 'ち': '32', 'ぢ': '33', 'っ': '34', 'つ': '35', 'づ': '36', 'て': '37', 'で': '38', 'と': '39', 'ど': '40', 'な': '41', 'に': '42', 'ぬ': '43', 'ね': '44', 'の': '45', 'は': '46', 'ば': '47', 'ぱ': '48', 'ひ': '49', 'び': '50', 'ぴ': '51', 'ふ': '52', 'ぶ': '53', 'ぷ': '54', 'へ': '55', 'べ': '56', 'ぺ': '57', 'ほ': '58', 'ぼ': '59', 'ぽ': '60', 'ま': '61', 'み': '62', 'む': '63', 'め': '64', 'も': '65', 'ゃ': '66', 'や': '67', 'ゅ': '68', 'ゆ': '69', 'ょ': '70', 'よ': '71', 'ら': '72', 'り': '73', 'る': '74', 'れ': '75', 'ろ': '76', 'ゎ': '77', 'わ': '78', 'ゐ': '79', 'ゑ': '80', 'を': '81', 'ん': '82', 'ゝ': '83', 'ゞ': '84'}\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "img_dicts = {}\n",
    "label_dicts = {}\n",
    "# key: あーん\n",
    "# value: [all paths]\n",
    "for cls, label in zip(all_label_classes, all_labels):\n",
    "  img_dicts[cls] = []\n",
    "  label_dicts[cls] = str(label)\n",
    "  # hiragana73_path\n",
    "  hiragana73_path_target_cls_dir = os.path.join(hiragana73_path, cls)\n",
    "  if os.path.exists(hiragana73_path_target_cls_dir):\n",
    "    hiragana73_cls_imgs = glob.glob(hiragana73_path_target_cls_dir + '/*')\n",
    "  else:\n",
    "    hiragana73_cls_imgs = []\n",
    "  print(len(hiragana73_cls_imgs))\n",
    "\n",
    "  # hiragana_imgs_path\n",
    "  hiragana_imgs_path_target_cls_dir = os.path.join(hiragana_imgs_path, cls)\n",
    "  if os.path.exists(hiragana_imgs_path_target_cls_dir):\n",
    "    hiragana_imgs_path_cls_imgs = glob.glob(hiragana_imgs_path_target_cls_dir + '/*')\n",
    "  else:\n",
    "    hiragana_imgs_path_cls_imgs = []\n",
    "  print(len(hiragana_imgs_path_cls_imgs))\n",
    "  \n",
    "  cls_imgs = hiragana73_cls_imgs + hiragana_imgs_path_cls_imgs\n",
    "  print(len(cls_imgs))\n",
    "\n",
    "  for cls_img_path in cls_imgs:\n",
    "    img_dicts[cls].append(cls_img_path)\n",
    "  print(cls)\n",
    "  print(len(img_dicts[cls]))\n",
    "\n",
    "print(label_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be0f5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112570, 2)\n",
      "[['../dataset/hiragana_imgs/ぁ/AP.jpg' '0']\n",
      " ['../dataset/hiragana_imgs/ぁ/ZinHenaKokuryu-RDF.jpg' '0']\n",
      " ['../dataset/hiragana_imgs/ぁ/851H-kktt_004.jpg' '0']\n",
      " ...\n",
      " ['../dataset/hiragana_imgs/ゞ/GenShinGothic-P-ExtraLight.jpg' '84']\n",
      " ['../dataset/hiragana_imgs/ゞ/GenEiMGothic2-Medium.jpg' '84']\n",
      " ['../dataset/hiragana_imgs/ゞ/GenEiGothicM-Bold.jpg' '84']]\n"
     ]
    }
   ],
   "source": [
    "# create train paths and label\n",
    "train_data = []\n",
    "\n",
    "for hira, fps in img_dicts.items():\n",
    "    label =int(label_dicts[hira])\n",
    "    for fp in fps:\n",
    "        train_data.append([fp, label])\n",
    "train_data = np.array(train_data)\n",
    "print(train_data.shape)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0d0585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: gra background to white and gray char to black\n",
    "# if create all black, then pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe18515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nenchan img\n",
    "# get 'ne, nn, ti, lya'\n",
    "ne_l = [d[0] for d in train_data if d[1] == label_dicts['ね']]\n",
    "nn_l = [d[0] for d in train_data if d[1] == label_dicts['ん']]\n",
    "ti_l = [d[0] for d in train_data if d[1] == label_dicts['ち']]\n",
    "lya_l = [d[0] for d in train_data if d[1] == label_dicts['ゃ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "148af626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_preprcess(fp):\n",
    "    img = cv2.imread(fp)\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = img_gray.sum() / (img_gray.shape[0] * img_gray.shape[1])\n",
    "    img_wb = cv2.threshold(img_gray, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    return img_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "604419e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 28, 140)\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "train_img = []\n",
    "data_num = 200000\n",
    "for i in range(data_num):\n",
    "    ne_c = img_preprcess(random.choice(ne_l))\n",
    "    nn_c1 = img_preprcess(random.choice(nn_l))\n",
    "    ti_c = img_preprcess(random.choice(ti_l))\n",
    "    lya_c = img_preprcess(random.choice(lya_l))\n",
    "    nn_c2 = img_preprcess(random.choice(nn_l))\n",
    "    con_img = cv2.hconcat([ne_c, nn_c1, ti_c, lya_c, nn_c2])\n",
    "    if (ne_c.sum() == 0) or (nn_c1.sum() == 0) or (ti_c.sum() == 0) or (lya_c.sum() == 0) or (nn_c2.sum() == 0):\n",
    "        print(\"has black img\")\n",
    "        cv2.imwrite('sample_black.jpg', con_img)\n",
    "        continue\n",
    "    train_img.append(con_img)\n",
    "    # time.sleep(0.5)\n",
    "    # cv2.imwrite('sample.jpg', con_img)\n",
    "cv2.imwrite('sample.jpg', con_img)\n",
    "train_img = np.array(train_img)\n",
    "print(train_img.shape)\n",
    "\n",
    "# background labelもあった方がいいかも？？？\n",
    "train_label = np.array([0] * len(train_img)).astype(np.uint8)\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6eea51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_img = []\\nfor fp in train_data[0]:\\n    img = cv2.imread(fp)\\n    img = cv2.resize(img, (28, 28))\\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n    train_img.append(img)\\ntrain_img = np.array(train_img)\\nprint(train_img.shape)\\ntrain_label = train_data[1].astype(np.uint8)\\nprint(train_label)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_img = []\n",
    "for fp in train_data[0]:\n",
    "    img = cv2.imread(fp)\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    train_img.append(img)\n",
    "train_img = np.array(train_img)\n",
    "print(train_img.shape)\n",
    "train_label = train_data[1].astype(np.uint8)\n",
    "print(train_label)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479de6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'np.random.shuffle(train_data)\\ntrain_data =  train_data.T\\nprint(train_data)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"np.random.shuffle(train_data)\n",
    "train_data =  train_data.T\n",
    "print(train_data)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mn5vRB0nE7GY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mn5vRB0nE7GY",
    "outputId": "947cf88b-1c7c-41de-cce8-2e4dce962fb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 28, 140)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7daf82e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3acc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e43df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fa348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d32f15e8",
   "metadata": {},
   "source": [
    "# start GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73c57177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "img_w, img_h = train_img[0].shape\n",
    "print(img_w)\n",
    "print(img_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ba50175",
   "metadata": {
    "id": "0ba50175"
   },
   "outputs": [],
   "source": [
    "class Transform(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample = np.array(sample, dtype = np.float32)\n",
    "        sample = torch.tensor(sample)\n",
    "        return (sample/127.5)-1\n",
    "\n",
    "transform = Transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4908fa4a",
   "metadata": {
    "id": "4908fa4a"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class dataset_full(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, img, label, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data_num = len(img)\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        for i in tqdm(range(self.data_num)):\n",
    "            self.data.append([img[i]])\n",
    "            self.label.append(label[i])\n",
    "        self.data_num = len(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = np.identity(num_label)[self.label[idx]]\n",
    "        out_label = np.array(out_label, dtype = np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "\n",
    "        return out_data, out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3dbc8717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 683053.00it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset_full(train_img, train_label, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5337a6e2",
   "metadata": {
    "id": "5337a6e2"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2759c4c7",
   "metadata": {
    "id": "2759c4c7"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, num_class):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim, 300)\n",
    "        self.bn1 = nn.BatchNorm1d(300)\n",
    "        self.LReLU1 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(num_class, 1500)\n",
    "        self.bn2 = nn.BatchNorm1d(1500)\n",
    "        self.LReLU2 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.fc3 = nn.Linear(1800, 128 * int((img_w / 4)) * int((img_h / 4)))\n",
    "        self.bn3 = nn.BatchNorm1d(128 * int((img_w / 4)) * int((img_h / 4)))\n",
    "        self.bo1 = nn.Dropout(p=0.5)\n",
    "        self.LReLU3 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), #チャネル数を128⇒64に変える。\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1), #チャネル数を64⇒1に変更\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.ConvTranspose2d):\n",
    "                module.weight.data.normal_(0, 0.02)\n",
    "                module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(0, 0.02)\n",
    "                module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                module.weight.data.normal_(1.0, 0.02)\n",
    "                module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.normal_(1.0, 0.02)\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        y_1 = self.fc1(noise)\n",
    "        y_1 = self.bn1(y_1)\n",
    "        y_1 = self.LReLU1(y_1)\n",
    "\n",
    "        y_2 = self.fc2(labels)\n",
    "        y_2 = self.bn2(y_2)\n",
    "        y_2 = self.LReLU2(y_2)\n",
    "\n",
    "        x = torch.cat([y_1, y_2], 1)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bo1(x)\n",
    "        x = self.LReLU3(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 128, int((img_w / 4)), int((img_h / 4)))\n",
    "        # print(x.shape)\n",
    "        x = self.deconv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0339bbe8",
   "metadata": {
    "id": "0339bbe8"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_class = num_class\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(num_class + 1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * int((img_w / 4)) * int((img_h / 4)), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                module.weight.data.normal_(0, 0.02)\n",
    "                module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(0, 0.02)\n",
    "                module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                module.weight.data.normal_(1.0, 0.02)\n",
    "                module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.normal_(1.0, 0.02)\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        y_2 = labels.view(-1, self.num_class, 1, 1)\n",
    "        y_2 = y_2.expand(-1, -1, img_w, img_h)\n",
    "\n",
    "        x = torch.cat([img, y_2], 1)\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 128 * int((img_w / 4)) * int((img_h / 4)))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e7f7908",
   "metadata": {
    "id": "2e7f7908"
   },
   "outputs": [],
   "source": [
    "def train_func(D_model, G_model, batch_size, z_dim, num_class, criterion, \n",
    "               D_optimizer, G_optimizer, data_loader, device):\n",
    "    #訓練モード\n",
    "    D_model.train()\n",
    "    G_model.train()\n",
    "\n",
    "    #本物のラベルは1\n",
    "    y_real = torch.ones((batch_size, 1)).to(device)\n",
    "    D_y_real = (torch.rand((batch_size, 1))/2 + 0.7).to(device) #Dに入れるノイズラベル\n",
    "\n",
    "    #偽物のラベルは0\n",
    "    y_fake = torch.zeros((batch_size, 1)).to(device)\n",
    "    D_y_fake = (torch.rand((batch_size, 1)) * 0.3).to(device) #Dに入れるノイズラベル\n",
    "\n",
    "    #lossの初期化\n",
    "    D_running_loss = 0\n",
    "    G_running_loss = 0\n",
    "\n",
    "    #バッチごとの計算\n",
    "    for batch_idx, (data, labels) in enumerate(data_loader):\n",
    "        #バッチサイズに満たない場合は無視\n",
    "        if data.size()[0] != batch_size:\n",
    "            break\n",
    "\n",
    "        #ノイズ作成\n",
    "        z = torch.normal(mean = 0.5, std = 0.2, size = (batch_size, z_dim)) #平均0.5の正規分布に従った乱数を生成\n",
    "\n",
    "        real_img, label, z = data.to(device), labels.to(device), z.to(device)\n",
    "\n",
    "        #Discriminatorの更新\n",
    "        D_optimizer.zero_grad()\n",
    "\n",
    "        #Discriminatorに本物画像を入れて順伝播⇒Loss計算\n",
    "        D_real = D_model(real_img, label)\n",
    "        D_real_loss = criterion(D_real, D_y_real)\n",
    "\n",
    "        #DiscriminatorにGeneratorにノイズを入れて作った画像を入れて順伝播⇒Loss計算\n",
    "        fake_img = G_model(z, label)\n",
    "        D_fake = D_model(fake_img.detach(), label) #fake_imagesで計算したLossをGeneratorに逆伝播させないように止める\n",
    "        D_fake_loss = criterion(D_fake, D_y_fake)\n",
    "\n",
    "        #2つのLossの和を最小化\n",
    "        D_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        D_running_loss += D_loss.item()\n",
    "\n",
    "        #Generatorの更新\n",
    "        G_optimizer.zero_grad()\n",
    "\n",
    "        #Generatorにノイズを入れて作った画像をDiscriminatorに入れて順伝播⇒見破られた分がLossになる\n",
    "        fake_img_2 = G_model(z, label)\n",
    "        D_fake_2 = D_model(fake_img_2, label)\n",
    "\n",
    "        #Gのloss(max(log D)で最適化)\n",
    "        G_loss = -criterion(D_fake_2, y_fake)\n",
    "\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        G_running_loss += G_loss.item()\n",
    "\n",
    "    D_running_loss /= len(data_loader)\n",
    "    G_running_loss /= len(data_loader)\n",
    "\n",
    "    return D_running_loss, G_running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34959ec4",
   "metadata": {
    "id": "34959ec4"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from torchvision.utils import save_image\n",
    "%matplotlib inline\n",
    "\n",
    "def Generate_img(epoch, G_model, device, z_dim, noise, var_mode, labels, log_dir='./logs_cGAN'):\n",
    "    G_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if var_mode == True:\n",
    "            #生成に必要な乱数\n",
    "            noise = torch.normal(mean = 0.5, std = 0.2, size = (num_label, z_dim)).to(device)\n",
    "        else:\n",
    "            noise = noise\n",
    "\n",
    "        #Generatorでサンプル生成\n",
    "        import pdb;pdb.set_trace()\n",
    "        samples = G_model(noise, labels).data.cpu()\n",
    "        samples = (samples/2)+0.5\n",
    "        try:\n",
    "          print(os.path.join(log_dir, 'epoch_%05d.png'))\n",
    "          print(os.path.exists(log_dir))\n",
    "          save_image(samples,os.path.join(log_dir, 'epoch_%05d.png' % (epoch)), nrow = 7)\n",
    "        except:\n",
    "          import pdb;pdb.set_trace()\n",
    "        img = Image('./logs_cGAN/epoch_%05d.png' % (epoch))\n",
    "        display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03cf31ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "03cf31ba",
    "outputId": "24d4b798-038a-4a75-8adb-55542c72ca53",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | 所要時間 5 分 57 秒\n",
      "\tLoss: 0.9382(Discriminator)\n",
      "\tLoss: -0.4288(Generator)\n",
      "> \u001b[0;32m<ipython-input-51-21b8f46055ea>\u001b[0m(18)\u001b[0;36mGenerate_img\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     16 \u001b[0;31m        \u001b[0;31m#Generatorでサンプル生成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 18 \u001b[0;31m        \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     19 \u001b[0;31m        \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     20 \u001b[0;31m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> noise\n",
      "tensor([[0.2787, 0.5323, 0.3630,  ..., 0.3289, 0.6153, 0.5144],\n",
      "        [0.6162, 0.6210, 0.3869,  ..., 0.8749, 0.9489, 1.0593],\n",
      "        [0.2456, 0.4697, 0.6470,  ..., 0.7699, 0.4950, 0.4187],\n",
      "        ...,\n",
      "        [0.7263, 0.2793, 0.5867,  ..., 0.4621, 0.3470, 0.4157],\n",
      "        [0.5121, 0.8131, 0.6434,  ..., 0.4014, 0.5223, 0.1114],\n",
      "        [0.6198, 0.4195, 0.5730,  ..., 0.5135, 0.4866, 0.0438]],\n",
      "       device='cuda:0')\n",
      "ipdb> noise.shape\n",
      "torch.Size([85, 30])\n",
      "ipdb> labels.shape\n",
      "torch.Size([85, 85])\n",
      "ipdb> labels\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-8f62ccc46fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m#モデルを回す\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mD_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-8f62ccc46fe2>\u001b[0m in \u001b[0;36mmodel_run\u001b[0;34m(num_epochs, batch_size, dataloader, device)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mGenerate_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m#モデル保存のためのcheckpointファイルを作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-21b8f46055ea>\u001b[0m in \u001b[0;36mGenerate_img\u001b[0;34m(epoch, G_model, device, z_dim, noise, var_mode, labels, log_dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#Generatorでサンプル生成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-21b8f46055ea>\u001b[0m in \u001b[0;36mGenerate_img\u001b[0;34m(epoch, G_model, device, z_dim, noise, var_mode, labels, log_dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#Generatorでサンプル生成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#再現性確保のためseed値固定\n",
    "SEED = 1111\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED) \n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def model_run(num_epochs, batch_size = batch_size, dataloader = train_loader, device = device):\n",
    "\n",
    "    #Generatorに入れるノイズの次元\n",
    "    z_dim = 30\n",
    "    var_mode = False #表示結果を見るときに毎回異なる乱数を使うかどうか\n",
    "    #生成に必要な乱数\n",
    "    noise = torch.normal(mean = 0.5, std = 0.2, size = (num_label, z_dim)).to(device)\n",
    "\n",
    "    #クラス数\n",
    "    num_class = num_label\n",
    "\n",
    "    #Generatorを試すときに使うラベルを作る\n",
    "    labels = []\n",
    "    for i in range(num_class):\n",
    "        tmp = np.identity(num_class)[i]\n",
    "        tmp = np.array(tmp, dtype = np.float32)\n",
    "        labels.append(tmp)\n",
    "    label = torch.Tensor(labels).to(device)\n",
    "\n",
    "    #モデル定義\n",
    "    D_model = Discriminator(num_class).to(device)\n",
    "    G_model = Generator(z_dim, num_class).to(device)\n",
    "\n",
    "    #lossの定義(引数はtrain_funcの中で指定)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    #optimizerの定義\n",
    "    D_optimizer = torch.optim.Adam(D_model.parameters(), lr=0.0002, betas=(0.5, 0.999), eps=1e-08, weight_decay=1e-5, amsgrad=False)\n",
    "    G_optimizer = torch.optim.Adam(G_model.parameters(), lr=0.0002, betas=(0.5, 0.999), eps=1e-08, weight_decay=1e-5, amsgrad=False)\n",
    "\n",
    "    D_loss_list = []\n",
    "    G_loss_list = []\n",
    "\n",
    "    all_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        D_loss, G_loss = train_func(D_model, G_model, batch_size, z_dim, num_class, criterion, \n",
    "                                    D_optimizer, G_optimizer, dataloader, device)\n",
    "\n",
    "        D_loss_list.append(D_loss)\n",
    "        G_loss_list.append(G_loss)\n",
    "\n",
    "        secs = int(time.time() - start_time)\n",
    "        mins = secs / 60\n",
    "        secs = secs % 60\n",
    "\n",
    "        #エポックごとに結果を表示\n",
    "        print('Epoch: %d' %(epoch + 1), \" | 所要時間 %d 分 %d 秒\" %(mins, secs))\n",
    "        print(f'\\tLoss: {D_loss:.4f}(Discriminator)')\n",
    "        print(f'\\tLoss: {G_loss:.4f}(Generator)')\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            Generate_img(epoch, G_model, device, z_dim, noise, var_mode, label)\n",
    "\n",
    "        #モデル保存のためのcheckpointファイルを作成\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch':epoch,\n",
    "                'model_state_dict':G_model.state_dict(),\n",
    "                'optimizer_state_dict':G_optimizer.state_dict(),\n",
    "                'loss':G_loss,\n",
    "            }, './checkpoint_cGAN/G_model_{}'.format(epoch + 1))\n",
    "\n",
    "    return D_loss_list, G_loss_list\n",
    "\n",
    "#モデルを回す\n",
    "D_loss_list, G_loss_list = model_run(num_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "int((img_h / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9224e1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "b9224e1f",
    "outputId": "9e8f2c3a-ccc7-42e8-8627-f9dda3844ac1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "\n",
    "loss = fig.add_subplot(1,1,1)\n",
    "\n",
    "loss.plot(range(len(D_loss_list)),D_loss_list,label='Discriminator_loss')\n",
    "loss.plot(range(len(G_loss_list)),G_loss_list,label='Generator_loss')\n",
    "\n",
    "loss.set_xlabel('epoch')\n",
    "loss.set_ylabel('loss')\n",
    "\n",
    "loss.legend()\n",
    "loss.grid()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feK3GCEl1kku",
   "metadata": {
    "id": "feK3GCEl1kku"
   },
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "%matplotlib inline\n",
    "#取り出すepochを指定する\n",
    "point = 15\n",
    "\n",
    "#モデルの構造を定義\n",
    "z_dim = 30\n",
    "num_class = num_label\n",
    "G = Generator(z_dim = z_dim, num_class = num_class)\n",
    "\n",
    "#checkpointを取り出す\n",
    "checkpoint = torch.load('./checkpoint_cGAN/G_model_{}'.format(point))\n",
    "\n",
    "#Generatorにパラメータを入れる\n",
    "G.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#検証モードにしておく\n",
    "G.eval()\n",
    "\n",
    "#pickleで保存\n",
    "with open ('nenchan_cGAN.pkl','wb')as f:\n",
    "    cloudpickle.dump(G,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed07e993",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 85 elements not 1500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-d80f63ee520a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# import pdb;pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/handwritten_text_cGAN-hyhnjs3F/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-69858ad9ac4c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, noise, labels)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0my_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0my_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0my_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLReLU2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/handwritten_text_cGAN-hyhnjs3F/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/handwritten_text_cGAN-hyhnjs3F/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/handwritten_text_cGAN-hyhnjs3F/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 85 elements not 1500"
     ]
    }
   ],
   "source": [
    "# letter = 'あいうえおかきくけこさしすせそたちつてとなにぬねのはひふへほまみむめもやゆよらりるれろわゐゑをんゝ'\n",
    "\n",
    "# strs = input()\n",
    "with open('nenchan_cGAN.pkl','rb')as f:\n",
    "    Generator = cloudpickle.load(f)\n",
    "\n",
    "# import pdb;pdb.set_trace()\n",
    "noise = torch.normal(mean = 0.5, std = 0.2, size = (1, 30))\n",
    "tmp = np.identity(num_label)\n",
    "tmp = np.array(tmp, dtype = np.float32)\n",
    "label = [tmp]\n",
    "\n",
    "img = Generator(noise, torch.Tensor(label))\n",
    "img = img.reshape((img_w,img_h))\n",
    "# import pdb;pdb.set_trace()\n",
    "img = img.detach().numpy()\n",
    "\n",
    "cv2.imwrite('./sentence.png', img)\n",
    "# img = Image('./sentence.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35ce7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
